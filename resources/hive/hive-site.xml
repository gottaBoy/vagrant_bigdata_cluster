<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
	<name>hive.metastore.local</name>
	<value>false</value>
	<description>使用本机mysql服务器存储元数据。这种存储方式需要在本地运行一个mysql服>务器</description>
    </property>
    <property>
	<name>system:java.io.tmpdir</name>
	<value>/opt/module/hive/tmpdir</value>
    </property>

    <property>
	<name>javax.jdo.option.ConnectionURL</name>
	<value>jdbc:mysql://hdp103:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
    </property>
    <property>
	<name>javax.jdo.option.ConnectionDriverName</name>
	<value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
	<name>javax.jdo.option.ConnectionUserName</name>
	<value>hive</value>
    </property>
    <property>
	<name>javax.jdo.option.ConnectionPassword</name>
	<value>hive</value>
	<description>password to use against metastore database</description>
    </property>
    <property>
	<name>hive.metastore.warehouse.dir</name>
	<value>/user/hive/warehouse</value>
    </property>
    <property>
	<name>datanucleus.autoCreateSchema</name>
	<value>false</value>
    </property>
    <property>
	<name>hive.metastore.schema.verification</name>
	<value>false</value>
    </property>
    <!--显示数据库名称-->
    <property>
	<name>hive.cli.print.current.db</name>
	<value>true</value>
    </property>
    <property>
	<name>hive.cli.print.header</name>
	<value>true</value>
    </property>
    <property>
	<name>hive.server2.thrift.port</name>
	<value>10000</value>
    </property>

    <property>
	<name>hive.server2.thrift.bind.host</name>
	<value>hdp101</value>
    </property>
    <property>
	<name>hive.metastore.event.db.notification.api.auth</name>
	<value>false</value>
    </property>
	<!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
	<!-- wget https://mirrors.huaweicloud.com/apache/spark/spark-3.0.0/spark-3.0.0-bin-without-hadoop.tgz -->
    <property>
	<name>spark.yarn.jars</name>
	<value>hdfs://hdp101:8020/spark-jars/*</value>
    </property>
	
    <!--Hive执行引擎-->
    <property>
	<name>hive.execution.engine</name>
	<value>spark</value>
    </property>
    <!-- HiveServer2启用Kerberos认证 -->
    <property>
        <name>hive.server2.authentication</name>
        <value>kerberos</value>
    </property>

    <!-- HiveServer2服务的Kerberos主体 -->
    <property>
        <name>hive.server2.authentication.kerberos.principal</name>
        <value>hive/hdp101@EXAMPLE.COM</value>
    </property>

    <!-- HiveServer2服务的Kerberos密钥文件 -->
    <property>
        <name>hive.server2.authentication.kerberos.keytab</name>
        <value>/etc/security/keytab/hive.service.keytab</value>
    </property>

    <!-- Metastore启动认证 -->
    <property>
        <name>hive.metastore.sasl.enabled</name>
        <value>true</value>
    </property>
    <!-- Metastore Kerberos密钥文件 -->
    <property>
        <name>hive.metastore.kerberos.keytab.file</name>
        <value>/etc/security/keytab/hive.service.keytab</value>
    </property>
    <!-- Metastore Kerberos主体 -->
    <property>
        <name>hive.metastore.kerberos.principal</name>
        <value>hive/hdp101@EXAMPLE.COM</value>
    </property>
    <property>
       <name>hive.exec.post.hooks</name>
       <value>org.apache.atlas.hive.hook.HiveHook</value>
    </property>
</configuration>

